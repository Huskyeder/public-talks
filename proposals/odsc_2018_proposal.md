# ODSC 2018 Proposal

## Title

Hands on w/ NLP: Predicting Spoilers w/ Deep Learning

## Abstract

Deep learning has drastically changed the way machines interact with human languages. From machine translation to 
textbook writing, Natural Language Processing (NLP) — the branch of ML focused on human language models — has gone from 
sci-fi to well understood.

However, translating academic papers to working, deployable code is still a huge task. From text preprocessing to model 
architectures to optimizers, learning rates, and other hyper-parameters, building an NLP model pipeline can be a 
daunting, heuristic filled process. 

Over the course of this talk, we'll follow the case study of predicting which Reddit submissions contain move spoilers 
to better understand common heuristics and best practices for building NLP models. This case study will then be 
followed by a larger discussion covering common Deep Learning architectures, and how linear approaches translate to 
Deep Learning approaches. 

Attendees should have a basic familiarity with Natural Language Processing and Deep Learning concepts, and will learn:

 - Common frameworks for transforming and modelling text data sets
 - Common Deep Learning architectures for NLP tasks
 - Common approaches to choose optimizers and estimate learning rates, network depth / width
 - How to approach NLP tasks with a new, concrete Deep Learning tool set and best practices.        

## Difficulty

## Suggestions

Originality
Topic Areas